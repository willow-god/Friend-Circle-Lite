import logging
from datetime import datetime, timedelta, timezone
from dateutil import parser
import requests
import re
import feedparser
from concurrent.futures import ThreadPoolExecutor, as_completed

# è®¾ç½®æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='ğŸ¤ª%(levelname)s: %(message)s')

# æ ‡å‡†åŒ–çš„è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
}

timeout = (10, 15) # è¿æ¥è¶…æ—¶å’Œè¯»å–è¶…æ—¶ï¼Œé˜²æ­¢requestsæ¥å—æ—¶é—´è¿‡é•¿

def format_published_time(time_str):
    """
    æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´ä¸ºç»Ÿä¸€æ ¼å¼ YYYY-MM-DD HH:MM

    å‚æ•°:
    time_str (str): è¾“å…¥çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯å¤šç§æ ¼å¼ã€‚

    è¿”å›:
    str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œè‹¥è§£æå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    # å°è¯•è‡ªåŠ¨è§£æè¾“å…¥æ—¶é—´å­—ç¬¦ä¸²
    try:
        parsed_time = parser.parse(time_str, fuzzy=True)
    except (ValueError, parser.ParserError):
        # å®šä¹‰æ”¯æŒçš„æ—¶é—´æ ¼å¼
        time_formats = [
            '%a, %d %b %Y %H:%M:%S %z',  # Mon, 11 Mar 2024 14:08:32 +0000
            '%a, %d %b %Y %H:%M:%S GMT',   # Wed, 19 Jun 2024 09:43:53 GMT
            '%Y-%m-%dT%H:%M:%S%z',         # 2024-03-11T14:08:32+00:00
            '%Y-%m-%dT%H:%M:%SZ',          # 2024-03-11T14:08:32Z
            '%Y-%m-%d %H:%M:%S',           # 2024-03-11 14:08:32
            '%Y-%m-%d'                     # 2024-03-11
        ]
        for fmt in time_formats:
            try:
                parsed_time = datetime.strptime(time_str, fmt)
                break
            except ValueError:
                continue
        else:
            logging.warning(f"æ— æ³•è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼š{time_str}")
            return ''

    # å¤„ç†æ—¶åŒºè½¬æ¢
    if parsed_time.tzinfo is None:
        parsed_time = parsed_time.replace(tzinfo=timezone.utc)
    shanghai_time = parsed_time.astimezone(timezone(timedelta(hours=8)))
    return shanghai_time.strftime('%Y-%m-%d %H:%M')



def check_feed(blog_url, session):
    """
    æ£€æŸ¥åšå®¢çš„ RSS æˆ– Atom è®¢é˜…é“¾æ¥ã€‚

    æ­¤å‡½æ•°æ¥å—ä¸€ä¸ªåšå®¢åœ°å€ï¼Œå°è¯•åœ¨å…¶åæ‹¼æ¥ '/atom.xml', '/rss2.xml' å’Œ '/feed'ï¼Œå¹¶æ£€æŸ¥è¿™äº›é“¾æ¥æ˜¯å¦å¯è®¿é—®ã€‚
    Atom ä¼˜å…ˆï¼Œå¦‚æœéƒ½ä¸èƒ½è®¿é—®ï¼Œåˆ™è¿”å› ['none', æºåœ°å€]ã€‚

    å‚æ•°ï¼š
    blog_url (str): åšå®¢çš„åŸºç¡€ URLã€‚
    session (requests.Session): ç”¨äºè¯·æ±‚çš„ä¼šè¯å¯¹è±¡ã€‚

    è¿”å›ï¼š
    list: åŒ…å«ç±»å‹å’Œæ‹¼æ¥åçš„é“¾æ¥çš„åˆ—è¡¨ã€‚å¦‚æœ atom é“¾æ¥å¯è®¿é—®ï¼Œåˆ™è¿”å› ['atom', atom_url]ï¼›
            å¦‚æœ rss2 é“¾æ¥å¯è®¿é—®ï¼Œåˆ™è¿”å› ['rss2', rss_url]ï¼›
            å¦‚æœ feed é“¾æ¥å¯è®¿é—®ï¼Œåˆ™è¿”å› ['feed', feed_url]ï¼›
            å¦‚æœéƒ½ä¸å¯è®¿é—®ï¼Œåˆ™è¿”å› ['none', blog_url]ã€‚
    """
    
    possible_feeds = [
        ('atom', '/atom.xml'),
        ('rss', '/rss.xml'), # 2024-07-26 æ·»åŠ  /rss.xmlå†…å®¹çš„æ”¯æŒ
        ('rss2', '/rss2.xml'),
        ('feed', '/feed'),
        ('feed2', '/feed.xml'), # 2024-07-26 æ·»åŠ  /feed.xmlå†…å®¹çš„æ”¯æŒ
        ('feed3', '/feed/'),
        ('index', '/index.xml') # 2024-07-25 æ·»åŠ  /index.xmlå†…å®¹çš„æ”¯æŒ
    ]

    for feed_type, path in possible_feeds:
        feed_url = blog_url.rstrip('/') + path
        try:
            response = session.get(feed_url, headers=headers, timeout=timeout)
            if response.status_code == 200:
                return [feed_type, feed_url]
        except requests.RequestException:
            continue
    logging.warning(f"æ— æ³•æ‰¾åˆ° {blog_url} çš„è®¢é˜…é“¾æ¥")
    return ['none', blog_url]


def parse_feed(url, session, count=5, blog_url=''):
    """
    è§£æ Atom æˆ– RSS2 feed å¹¶è¿”å›åŒ…å«ç½‘ç«™åç§°ã€ä½œè€…ã€åŸé“¾æ¥å’Œæ¯ç¯‡æ–‡ç« è¯¦ç»†å†…å®¹çš„å­—å…¸ã€‚

    æ­¤å‡½æ•°æ¥å—ä¸€ä¸ª feed çš„åœ°å€ï¼ˆatom.xml æˆ– rss2.xmlï¼‰ï¼Œè§£æå…¶ä¸­çš„æ•°æ®ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ç»“æ„ï¼Œ
    å…¶ä¸­åŒ…æ‹¬ç½‘ç«™åç§°ã€ä½œè€…ã€åŸé“¾æ¥å’Œæ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹ã€‚

    å‚æ•°ï¼š
    url (str): Atom æˆ– RSS2 feed çš„ URLã€‚
    session (requests.Session): ç”¨äºè¯·æ±‚çš„ä¼šè¯å¯¹è±¡ã€‚
    count (int): è·å–æ–‡ç« æ•°çš„æœ€å¤§æ•°ã€‚å¦‚æœå°äºåˆ™å…¨éƒ¨è·å–ï¼Œå¦‚æœæ–‡ç« æ•°å¤§äºåˆ™åªå–å‰ count ç¯‡æ–‡ç« ã€‚

    è¿”å›ï¼š
    dict: åŒ…å«ç½‘ç«™åç§°ã€ä½œè€…ã€åŸé“¾æ¥å’Œæ¯ç¯‡æ–‡ç« è¯¦ç»†å†…å®¹çš„å­—å…¸ã€‚
    """
    try:
        response = session.get(url, headers=headers, timeout=timeout)
        response.encoding = 'utf-8'
        feed = feedparser.parse(response.text)
        
        result = {
            'website_name': feed.feed.title if 'title' in feed.feed else '',
            'author': feed.feed.author if 'author' in feed.feed else '',
            'link': feed.feed.link if 'link' in feed.feed else '',
            'articles': []
        }
        
        for _ , entry in enumerate(feed.entries):
            
            if 'published' in entry:
                published = format_published_time(entry.published)
            elif 'updated' in entry:
                published = format_published_time(entry.updated)
                # è¾“å‡ºè­¦å‘Šä¿¡æ¯
                logging.warning(f"æ–‡ç«  {entry.title} æœªåŒ…å«å‘å¸ƒæ—¶é—´ï¼Œå·²ä½¿ç”¨æ›´æ–°æ—¶é—´ {published}")
            else:
                published = ''
                logging.warning(f"æ–‡ç«  {entry.title} æœªåŒ…å«ä»»ä½•æ—¶é—´ä¿¡æ¯, è¯·æ£€æŸ¥åŸæ–‡, è®¾ç½®ä¸ºé»˜è®¤æ—¶é—´")
            
            # å¤„ç†é“¾æ¥ä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯ï¼Œæ¯”å¦‚ipæˆ–localhost
            article_link = replace_non_domain(entry.link, blog_url) if 'link' in entry else ''
            
            article = {
                'title': entry.title if 'title' in entry else '',
                'author': result['author'],
                'link': article_link,
                'published': published,
                'summary': entry.summary if 'summary' in entry else '',
                'content': entry.content[0].value if 'content' in entry and entry.content else entry.description if 'description' in entry else ''
            }
            result['articles'].append(article)
        
        # å¯¹æ–‡ç« æŒ‰æ—¶é—´æ’åºï¼Œå¹¶åªå–å‰ count ç¯‡æ–‡ç« 
        result['articles'] = sorted(result['articles'], key=lambda x: datetime.strptime(x['published'], '%Y-%m-%d %H:%M'), reverse=True)
        if count < len(result['articles']):
            result['articles'] = result['articles'][:count]
        
        return result
    except Exception as e:
        logging.error(f"æ— æ³•è§£æFEEDåœ°å€ï¼š{url} ï¼Œè¯·è‡ªè¡Œæ’æŸ¥åŸå› ï¼")
        return {
            'website_name': '',
            'author': '',
            'link': '',
            'articles': []
        }

def replace_non_domain(link: str, blog_url: str) -> str:
    """
    æš‚æœªå®ç°
    æ£€æµ‹å¹¶æ›¿æ¢å­—ç¬¦ä¸²ä¸­çš„éæ­£å¸¸åŸŸåéƒ¨åˆ†ï¼ˆå¦‚ IP åœ°å€æˆ– localhostï¼‰ï¼Œæ›¿æ¢ä¸º blog_urlã€‚
    æ›¿æ¢åå¼ºåˆ¶ä½¿ç”¨ httpsï¼Œä¸”è€ƒè™‘ blog_url å°¾éƒ¨æ˜¯å¦æœ‰æ–œæ ã€‚

    :param link: åŸå§‹åœ°å€å­—ç¬¦ä¸²
    :param blog_url: æ›¿æ¢ä¸ºçš„åšå®¢åœ°å€
    :return: æ›¿æ¢åçš„åœ°å€å­—ç¬¦ä¸²
    """
    
    # æå–linkä¸­çš„è·¯å¾„éƒ¨åˆ†ï¼Œæ— éœ€åè®®å’ŒåŸŸå
    # path = re.sub(r'^https?://[^/]+', '', link)
    # print(path)
    
    return link

def process_friend(friend, session, count, specific_RSS=[]):
    """
    å¤„ç†å•ä¸ªæœ‹å‹çš„åšå®¢ä¿¡æ¯ã€‚

    å‚æ•°ï¼š
    friend (list): åŒ…å«æœ‹å‹ä¿¡æ¯çš„åˆ—è¡¨ [name, blog_url, avatar]ã€‚
    session (requests.Session): ç”¨äºè¯·æ±‚çš„ä¼šè¯å¯¹è±¡ã€‚
    count (int): è·å–æ¯ä¸ªåšå®¢çš„æœ€å¤§æ–‡ç« æ•°ã€‚
    specific_RSS (list): åŒ…å«ç‰¹å®š RSS æºçš„å­—å…¸åˆ—è¡¨ [{name, url}]

    è¿”å›ï¼š
    dict: åŒ…å«æœ‹å‹åšå®¢ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    name, blog_url, avatar = friend
    
    # å¦‚æœ specific_RSS ä¸­æœ‰å¯¹åº”çš„ nameï¼Œåˆ™ç›´æ¥è¿”å› feed_url
    if specific_RSS is None:
        specific_RSS = []
    rss_feed = next((rss['url'] for rss in specific_RSS if rss['name'] == name), None)
    if rss_feed:
        feed_url = rss_feed
        feed_type = 'specific'
        logging.info(f"â€œ{name}â€çš„åšå®¢â€œ {blog_url} â€ä¸ºç‰¹å®šRSSæºâ€œ {feed_url} â€")
    else:
        feed_type, feed_url = check_feed(blog_url, session)
        logging.info(f"â€œ{name}â€çš„åšå®¢â€œ {blog_url} â€çš„feedç±»å‹ä¸ºâ€œ{feed_type}â€, feedåœ°å€ä¸ºâ€œ {feed_url} â€")

    if feed_type != 'none':
        feed_info = parse_feed(feed_url, session, count, blog_url)
        articles = [
            {
                'title': article['title'],
                'created': article['published'],
                'link': article['link'],
                'author': name,
                'avatar': avatar
            }
            for article in feed_info['articles']
        ]
        
        for article in articles:
            logging.info(f"{name} å‘å¸ƒäº†æ–°æ–‡ç« ï¼š{article['title']}ï¼Œæ—¶é—´ï¼š{article['created']}ï¼Œé“¾æ¥ï¼š{article['link']}")
        
        return {
            'name': name,
            'status': 'active',
            'articles': articles
        }
    else:
        logging.warning(f"{name} çš„åšå®¢ {blog_url} æ— æ³•è®¿é—®")
        return {
            'name': name,
            'status': 'error',
            'articles': []
        }

def fetch_and_process_data(json_url, specific_RSS=[], count=5):
    """
    è¯»å– JSON æ•°æ®å¹¶å¤„ç†è®¢é˜…ä¿¡æ¯ï¼Œè¿”å›ç»Ÿè®¡æ•°æ®å’Œæ–‡ç« ä¿¡æ¯ã€‚

    å‚æ•°ï¼š
    json_url (str): åŒ…å«æœ‹å‹ä¿¡æ¯çš„ JSON æ–‡ä»¶çš„ URLã€‚
    count (int): è·å–æ¯ä¸ªåšå®¢çš„æœ€å¤§æ–‡ç« æ•°ã€‚
    specific_RSS (list): åŒ…å«ç‰¹å®š RSS æºçš„å­—å…¸åˆ—è¡¨ [{name, url}]

    è¿”å›ï¼š
    dict: åŒ…å«ç»Ÿè®¡æ•°æ®å’Œæ–‡ç« ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    session = requests.Session()
    
    try:
        response = session.get(json_url, headers=headers, timeout=timeout)
        friends_data = response.json()
    except Exception as e:
        logging.error(f"æ— æ³•è·å–é“¾æ¥ï¼š{json_url} ï¼š{e}", exc_info=True)
        return None

    total_friends = len(friends_data['friends'])
    active_friends = 0
    error_friends = 0
    total_articles = 0
    article_data = []
    error_friends_info = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_friend = {
            executor.submit(process_friend, friend, session, count, specific_RSS): friend
            for friend in friends_data['friends']
        }
        
        for future in as_completed(future_to_friend):
            friend = future_to_friend[future]
            try:
                result = future.result()
                if result['status'] == 'active':
                    active_friends += 1
                    article_data.extend(result['articles'])
                    total_articles += len(result['articles'])
                else:
                    error_friends += 1
                    error_friends_info.append(friend)
            except Exception as e:
                logging.error(f"å¤„ç† {friend} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                error_friends += 1
                error_friends_info.append(friend)

    result = {
        'statistical_data': {
            'friends_num': total_friends,
            'active_num': active_friends,
            'error_num': error_friends,
            'article_num': total_articles,
            'last_updated_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        },
        'article_data': article_data
    }
    
    logging.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œæ€»å…±æœ‰ {total_friends} ä½æœ‹å‹ï¼Œå…¶ä¸­ {active_friends} ä½åšå®¢å¯è®¿é—®ï¼Œ{error_friends} ä½åšå®¢æ— æ³•è®¿é—®")

    return result, error_friends_info

def sort_articles_by_time(data):
    """
    å¯¹æ–‡ç« æ•°æ®æŒ‰æ—¶é—´æ’åº

    å‚æ•°ï¼š
    data (dict): åŒ…å«æ–‡ç« ä¿¡æ¯çš„å­—å…¸

    è¿”å›ï¼š
    dict: æŒ‰æ—¶é—´æ’åºåçš„æ–‡ç« ä¿¡æ¯å­—å…¸
    """
    # å…ˆç¡®ä¿æ¯ä¸ªå…ƒç´ å­˜åœ¨æ—¶é—´
    for article in data['article_data']:
        if article['created'] == '' or article['created'] == None:
            article['created'] = '2024-01-01 00:00'
            # è¾“å‡ºè­¦å‘Šä¿¡æ¯
            logging.warning(f"æ–‡ç«  {article['title']} æœªåŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œå·²è®¾ç½®ä¸ºé»˜è®¤æ—¶é—´ 2024-01-01 00:00")
    
    if 'article_data' in data:
        sorted_articles = sorted(
            data['article_data'],
            key=lambda x: datetime.strptime(x['created'], '%Y-%m-%d %H:%M'),
            reverse=True
        )
        data['article_data'] = sorted_articles
    return data

def marge_data_from_json_url(data, marge_json_url):
    """
    ä»å¦ä¸€ä¸ª JSON æ–‡ä»¶ä¸­è·å–æ•°æ®å¹¶åˆå¹¶åˆ°åŸæ•°æ®ä¸­ã€‚

    å‚æ•°ï¼š
    data (dict): åŒ…å«æ–‡ç« ä¿¡æ¯çš„å­—å…¸
    marge_json_url (str): åŒ…å«å¦ä¸€ä¸ªæ–‡ç« ä¿¡æ¯çš„ JSON æ–‡ä»¶çš„ URLã€‚

    è¿”å›ï¼š
    dict: åˆå¹¶åçš„æ–‡ç« ä¿¡æ¯å­—å…¸ï¼Œå·²å»é‡å¤„ç†
    """
    try:
        response = requests.get(marge_json_url, headers=headers, timeout=timeout)
        marge_data = response.json()
    except Exception as e:
        logging.error(f"æ— æ³•è·å–é“¾æ¥ï¼š{marge_json_url}ï¼Œå‡ºç°çš„é—®é¢˜ä¸ºï¼š{e}", exc_info=True)
        return data
    
    if 'article_data' in marge_data:
        logging.info(f"å¼€å§‹åˆå¹¶æ•°æ®ï¼ŒåŸæ•°æ®å…±æœ‰ {len(data['article_data'])} ç¯‡æ–‡ç« ï¼Œç¬¬ä¸‰æ–¹æ•°æ®å…±æœ‰ {len(marge_data['article_data'])} ç¯‡æ–‡ç« ")
        data['article_data'].extend(marge_data['article_data'])
        data['article_data'] = list({v['link']:v for v in data['article_data']}.values())
        logging.info(f"åˆå¹¶æ•°æ®å®Œæˆï¼Œç°åœ¨å…±æœ‰ {len(data['article_data'])} ç¯‡æ–‡ç« ")
    return data

import requests

def marge_errors_from_json_url(errors, marge_json_url):
    """
    ä»å¦ä¸€ä¸ªç½‘ç»œ JSON æ–‡ä»¶ä¸­è·å–é”™è¯¯ä¿¡æ¯å¹¶éå†ï¼Œåˆ é™¤åœ¨errorsä¸­ï¼Œ
    ä¸å­˜åœ¨äºmarge_errorsä¸­çš„å‹é“¾ä¿¡æ¯ã€‚

    å‚æ•°ï¼š
    errors (list): åŒ…å«é”™è¯¯ä¿¡æ¯çš„åˆ—è¡¨
    marge_json_url (str): åŒ…å«å¦ä¸€ä¸ªé”™è¯¯ä¿¡æ¯çš„ JSON æ–‡ä»¶çš„ URLã€‚

    è¿”å›ï¼š
    list: åˆå¹¶åçš„é”™è¯¯ä¿¡æ¯åˆ—è¡¨
    """
    try:
        response = requests.get(marge_json_url, timeout=10)  # è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´
        marge_errors = response.json()
    except Exception as e:
        logging.error(f"æ— æ³•è·å–é“¾æ¥ï¼š{marge_json_url}ï¼Œå‡ºç°çš„é—®é¢˜ä¸ºï¼š{e}", exc_info=True)
        return errors

    # æå– marge_errors ä¸­çš„ URL
    marge_urls = {item[1] for item in marge_errors}

    # ä½¿ç”¨è¿‡æ»¤å™¨ä¿ç•™ errors ä¸­åœ¨ marge_errors ä¸­å‡ºç°çš„ URL
    filtered_errors = [error for error in errors if error[1] in marge_urls]

    logging.info(f"åˆå¹¶é”™è¯¯ä¿¡æ¯å®Œæˆï¼Œåˆå¹¶åå…±æœ‰ {len(filtered_errors)} ä½æœ‹å‹")
    return filtered_errors

def deal_with_large_data(result):
    """
    å¤„ç†æ–‡ç« æ•°æ®ï¼Œä¿ç•™å‰150ç¯‡åŠå…¶ä½œè€…åœ¨åç»­æ–‡ç« ä¸­çš„å‡ºç°ã€‚
    
    å‚æ•°ï¼š
    result (dict): åŒ…å«ç»Ÿè®¡æ•°æ®å’Œæ–‡ç« æ•°æ®çš„å­—å…¸ã€‚
    
    è¿”å›ï¼š
    dict: å¤„ç†åçš„æ•°æ®ï¼ŒåªåŒ…å«éœ€è¦çš„æ–‡ç« ã€‚
    """
    result = sort_articles_by_time(result)
    article_data = result.get("article_data", [])

    # æ£€æŸ¥æ–‡ç« æ•°é‡æ˜¯å¦å¤§äº 150
    max_articles = 150
    if len(article_data) > max_articles:
        logging.info("æ•°æ®é‡è¾ƒå¤§ï¼Œå¼€å§‹è¿›è¡Œå¤„ç†...")
        # è·å–å‰ max_articles ç¯‡æ–‡ç« çš„ä½œè€…é›†åˆ
        top_authors = {article["author"] for article in article_data[:max_articles]}

        # ä»ç¬¬ {max_articles + 1} ç¯‡å¼€å§‹è¿‡æ»¤ï¼Œåªä¿ç•™å‰ max_articles ç¯‡å‡ºç°è¿‡çš„ä½œè€…çš„æ–‡ç« 
        filtered_articles = article_data[:max_articles] + [
            article for article in article_data[max_articles:]
            if article["author"] in top_authors
        ]

        # æ›´æ–°ç»“æœä¸­çš„ article_data
        result["article_data"] = filtered_articles
        # æ›´æ–°ç»“æœä¸­çš„ç»Ÿè®¡æ•°æ®
        result["statistical_data"]["article_num"] = len(filtered_articles)
        logging.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œä¿ç•™ {len(filtered_articles)} ç¯‡æ–‡ç« ")

    return result